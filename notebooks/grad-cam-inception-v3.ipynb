{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"},{"sourceId":13462010,"sourceType":"datasetVersion","datasetId":8545108}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# Cell 1: Reproducibility\n# =============================================================================\nSEED = 5\nimport os, random\nimport numpy as np\nimport tensorflow as tf\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:05:50.828539Z","iopub.execute_input":"2025-10-22T06:05:50.828728Z","iopub.status.idle":"2025-10-22T06:06:04.224769Z","shell.execute_reply.started":"2025-10-22T06:05:50.828709Z","shell.execute_reply":"2025-10-22T06:06:04.223991Z"}},"outputs":[{"name":"stderr","text":"2025-10-22 06:05:52.339122: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761113152.538380      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761113152.597615      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Cell 2: Core Imports and Constants - ENHANCED VERSION\n# =============================================================================\nimport json, math, os, gc\nimport cv2\nfrom PIL import Image\nimport pickle\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import (\n    EfficientNetV2M, EfficientNetB5, InceptionV3, Xception, DenseNet169,\n    DenseNet121, InceptionResNetV2, ResNet50\n)\n# Machine Learning\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\nfrom sklearn.ensemble import VotingClassifier\nimport catboost as cb\nimport scipy\nfrom scipy.special import softmax\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Enhanced visualization imports\nimport seaborn as sns\nfrom matplotlib.colors import LinearSegmentedColormap\nimport matplotlib.patches as patches\nfrom matplotlib.gridspec import GridSpec\n\n# XAI Libraries\nimport shap\nimport lime\nfrom lime import lime_image\nfrom lime.wrappers.scikit_image import SegmentationAlgorithm\n\n# Jupyter\n%matplotlib inline\n\n# Constants - UPDATED BATCH SIZE FOR EFFICIENT MODELS\nIMG_SIZE = 224\nBATCH_SIZE = 16  # Reduced from 24 to 16 for EfficientNet models (they're memory intensive)\nEPOCHS = 50\nPATIENCE = 7\n\n# Class labels for diabetic retinopathy\nCLASS_LABELS = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n\n# Color palette for visualizations\nCOLORS = ['#2E8B57', '#FFD700', '#FF8C00', '#FF4500', '#DC143C']  # Green to Red gradient\nCMAP_CUSTOM = LinearSegmentedColormap.from_list('custom', ['#ffffff', '#1f77b4', '#0d47a1'], N=256)\n\n# GPU Memory Configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"‚úÖ GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"GPU configuration error: {e}\")\n\ntf.config.optimizer.set_jit(True)\nprint(\"‚úÖ XLA compilation enabled for memory efficiency\")\n\n# Kaggle input paths\nAPTOS_DIR = \"/kaggle/input/aptos2019-blindness-detection\"\nPROCESSED_DATA_PATH = \"/kaggle/input/pilise/preprocessed_aptos_data.npz\"\nTEST_CSV_PATH = \"/kaggle/input/pilise/test_data.csv\"\nTRAIN_CSV_PATH = \"/kaggle/input/pilise/train_data.csv\"\nprint(\"‚úÖ All imports completed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:07:08.221711Z","iopub.execute_input":"2025-10-22T06:07:08.222296Z","iopub.status.idle":"2025-10-22T06:07:08.234451Z","shell.execute_reply.started":"2025-10-22T06:07:08.222272Z","shell.execute_reply":"2025-10-22T06:07:08.233673Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU memory growth enabled for 1 GPU(s)\n‚úÖ XLA compilation enabled for memory efficiency\n‚úÖ All imports completed successfully\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 3: Load Preprocessed Data\n# =============================================================================\nprint(\"Loading preprocessed data...\")\ndata = np.load(PROCESSED_DATA_PATH)\nprint(\"Available keys in preprocessed data:\", list(data.keys()))\n\nx_train_full = data['X_train']\ny_train_full = data['y_train']\nx_test = data['X_test']  \ny_test = data['y_test']\n\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ntest_df = pd.read_csv(TEST_CSV_PATH)\n\nprint(f\"Training data shape: {x_train_full.shape}\")\nprint(f\"Training labels shape: {y_train_full.shape}\")\nprint(f\"Test data shape: {x_test.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")\n\n# Store original labels for metrics calculation\ny_test_original = y_test.copy() if len(y_test.shape) == 1 else np.argmax(y_test, axis=1)\ny_train_original = y_train_full.copy() if len(y_train_full.shape) == 1 else np.argmax(y_train_full, axis=1)\n\nprint(f\"Original label shapes - Train: {y_train_full.shape}, Test: {y_test.shape}\")\nif len(y_train_full.shape) == 1 or (len(y_train_full.shape) == 2 and y_train_full.shape[1] == 1):\n    print(\"Converting training labels to one-hot encoding...\")\n    from tensorflow.keras.utils import to_categorical\n    y_train_original = y_train_full.copy() if len(y_train_full.shape) == 1 else y_train_full.flatten()\n    y_train_full = to_categorical(y_train_full, num_classes=5)\n    print(f\"New training labels shape: {y_train_full.shape}\")\n\nif len(y_test.shape) == 1 or (len(y_test.shape) == 2 and y_test.shape[1] == 1):\n    print(\"Converting test labels to one-hot encoding...\")\n    from tensorflow.keras.utils import to_categorical\n    y_test_original = y_test.copy() if len(y_test.shape) == 1 else y_test.flatten()\n    y_test = to_categorical(y_test, num_classes=5)\n    print(f\"New test labels shape: {y_test.shape}\")\n\nprint(f\"Final shapes - Train: {y_train_full.shape}, Test: {y_test.shape}\")\nprint(f\"Label ranges - Train: [{y_train_full.min():.2f}, {y_train_full.max():.2f}], Test: [{y_test.min():.2f}, {y_test.max():.2f}]\")\n\n# Normalize pixel values\nif x_train_full.max() > 1.0:\n    print(\"Normalizing pixel values to [0,1]...\")\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\nelse:\n    print(\"Data already normalized\")\n    x_train_full = x_train_full.astype('float32')\n    x_test = x_test.astype('float32')\n\nprint(\"‚úÖ Data loaded and preprocessed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T06:07:14.375568Z","iopub.execute_input":"2025-10-22T06:07:14.376058Z","iopub.status.idle":"2025-10-22T06:07:30.343249Z","shell.execute_reply.started":"2025-10-22T06:07:14.376033Z","shell.execute_reply":"2025-10-22T06:07:30.342603Z"}},"outputs":[{"name":"stdout","text":"Loading preprocessed data...\nAvailable keys in preprocessed data: ['X_train', 'y_train', 'X_test', 'y_test']\nTraining data shape: (7220, 224, 224, 3)\nTraining labels shape: (7220, 5)\nTest data shape: (733, 224, 224, 3)\nTest labels shape: (733,)\nOriginal label shapes - Train: (7220, 5), Test: (733,)\nConverting test labels to one-hot encoding...\nNew test labels shape: (733, 5)\nFinal shapes - Train: (7220, 5), Test: (733, 5)\nLabel ranges - Train: [0.00, 1.00], Test: [0.00, 1.00]\nNormalizing pixel values to [0,1]...\n‚úÖ Data loaded and preprocessed successfully\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# Grad-CAM Visualization Script for Diabetic Retinopathy Models\n# FULLY REWRITTEN: Avoids Sequential model issues by using Functional API\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"GRAD-CAM VISUALIZATION SCRIPT - FUNCTIONAL API VERSION\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model, Model\nimport os\n\n# =============================================================================\n# Define Class Labels\n# =============================================================================\n\nCLASS_LABELS = {\n    0: 'No DR',\n    1: 'Mild',\n    2: 'Moderate', \n    3: 'Severe',\n    4: 'Proliferative DR'\n}\n\nprint(\"‚úÖ Class labels defined:\")\nfor idx, label in CLASS_LABELS.items():\n    print(f\"   {idx}: {label}\")\n\n# =============================================================================\n# Cell 1: Grad-CAM Implementation (FUNCTIONAL API APPROACH)\n# =============================================================================\n\ndef get_last_conv_layer_name(model):\n    \"\"\"\n    Automatically find the last convolutional layer in the model\n    \"\"\"\n    # Check main model layers first\n    for layer in reversed(model.layers):\n        if 'Conv' in layer.__class__.__name__:\n            return layer.name\n    \n    # Check inside base model if exists\n    if hasattr(model.layers[0], 'layers'):\n        for layer in reversed(model.layers[0].layers):\n            if 'Conv' in layer.__class__.__name__:\n                return layer.name\n    \n    raise ValueError(\"Could not find convolutional layer.\")\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"\n    Generate Grad-CAM heatmap using direct layer access (NO Sequential issues)\n    This approach extracts the base model and accesses layers directly\n    \"\"\"\n    # Get the base model (assumes model structure: Sequential([base_model, other_layers]))\n    if hasattr(model.layers[0], 'layers'):\n        base_model = model.layers[0]\n    else:\n        base_model = model\n    \n    # Get the target conv layer\n    try:\n        conv_layer = base_model.get_layer(last_conv_layer_name)\n    except:\n        # Fallback if layer not in base model\n        conv_layer = model.get_layer(last_conv_layer_name)\n    \n    # Create a new functional model: input -> [conv_output, final_predictions]\n    # This completely bypasses the Sequential model structure\n    try:\n        # Try to create model using base model structure\n        grad_model = Model(\n            inputs=base_model.input,\n            outputs=[conv_layer.output, base_model.output]\n        )\n        use_base = True\n    except:\n        # Fallback to full model\n        grad_model = Model(\n            inputs=model.input,\n            outputs=[conv_layer.output, model.output]\n        )\n        use_base = False\n    \n    # Compute gradients\n    with tf.GradientTape() as tape:\n        # Forward pass through grad_model\n        if use_base:\n            # Need to complete the forward pass through remaining layers\n            conv_outputs, base_predictions = grad_model(img_array)\n            \n            # Pass through remaining model layers (after base_model)\n            x = base_predictions\n            for layer in model.layers[1:]:\n                x = layer(x)\n            predictions = x\n        else:\n            conv_outputs, predictions = grad_model(img_array)\n        \n        # Get predicted class\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        \n        # Get loss for target class\n        class_channel = predictions[:, pred_index]\n    \n    # Compute gradients\n    grads = tape.gradient(class_channel, conv_outputs)\n    \n    if grads is None:\n        raise ValueError(f\"Gradients are None for layer {last_conv_layer_name}\")\n    \n    # Global average pooling\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    \n    # Weight channels and sum\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    # Normalize\n    heatmap = tf.maximum(heatmap, 0)\n    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-10)\n    \n    return heatmap.numpy()\n\n\ndef overlay_gradcam(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    \"\"\"\n    Overlay Grad-CAM heatmap on original image\n    \"\"\"\n    # Resize heatmap\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    \n    # Convert to RGB\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    \n    # Convert image to uint8\n    img_uint8 = np.uint8(255 * img)\n    \n    # Superimpose\n    superimposed_img = heatmap * alpha + img_uint8\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n    \n    return superimposed_img\n\n\n# =============================================================================\n# Cell 2: Select 50 Images\n# =============================================================================\n\ndef select_images_per_class(x_data, y_data, num_per_class=10, random_seed=42):\n    \"\"\"\n    Select specified number of images per class\n    \"\"\"\n    np.random.seed(random_seed)\n    \n    # Convert one-hot to integer if needed\n    if len(y_data.shape) > 1:\n        labels = np.argmax(y_data, axis=1)\n    else:\n        labels = y_data\n    \n    selected_indices = []\n    \n    for class_idx in range(5):\n        class_indices = np.where(labels == class_idx)[0]\n        \n        if len(class_indices) >= num_per_class:\n            selected = np.random.choice(class_indices, num_per_class, replace=False)\n        else:\n            selected = class_indices\n            print(f\"Warning: Only {len(class_indices)} images for class {class_idx}\")\n        \n        selected_indices.extend(selected)\n    \n    selected_indices = np.array(selected_indices)\n    selected_images = x_data[selected_indices]\n    selected_labels = labels[selected_indices]\n    \n    return selected_indices, selected_images, selected_labels\n\n\nprint(\"\\nüìä Selecting 50 images (10 per class)...\")\nselected_indices, selected_images, selected_labels = select_images_per_class(\n    x_test, y_test, num_per_class=10, random_seed=42\n)\n\nprint(f\"‚úÖ Selected {len(selected_indices)} images\")\nprint(f\"   Class distribution: {np.bincount(selected_labels)}\")\n\n\n# =============================================================================\n# Cell 3: Model Configuration\n# =============================================================================\n\nMODEL_PATHS = {\n    'EfficientNetV2M': '/kaggle/working/effnetv2m_final.h5',\n    'DenseNet169': '/kaggle/working/densenet169_best.h5',\n    'InceptionV3': '/kaggle/working/inceptionv3_final.h5',\n    'EfficientNetB5': '/kaggle/working/effnetb5_final.h5'\n}\n\n# Check available models\navailable_models = {}\nfor model_name, model_path in MODEL_PATHS.items():\n    if os.path.exists(model_path):\n        available_models[model_name] = model_path\n        print(f\"‚úÖ Found: {model_name}\")\n    else:\n        print(f\"‚ö†Ô∏è  Not found: {model_name}\")\n\nif len(available_models) == 0:\n    print(\"\\n‚ùå No models found!\")\nelse:\n    print(f\"\\n‚úÖ Found {len(available_models)} model(s)\")\n\n\n# =============================================================================\n# Cell 4: Generate Grad-CAM\n# =============================================================================\n\ndef generate_gradcam_for_model(model_name, model_path, images, labels):\n    \"\"\"\n    Generate Grad-CAM visualizations\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Processing: {model_name}\")\n    print(f\"{'='*80}\")\n    \n    # Load model\n    print(f\"Loading model...\")\n    model = load_model(model_path)\n    print(f\"   ‚úì Model loaded\")\n    \n    # Print model structure for debugging\n    print(f\"   Model type: {type(model)}\")\n    print(f\"   Number of layers: {len(model.layers)}\")\n    if hasattr(model.layers[0], 'layers'):\n        print(f\"   Base model: {type(model.layers[0])}\")\n        print(f\"   Base model layers: {len(model.layers[0].layers)}\")\n    \n    # Find conv layer\n    try:\n        last_conv_layer = get_last_conv_layer_name(model)\n        print(f\"‚úÖ Using layer: {last_conv_layer}\")\n    except ValueError as e:\n        print(f\"‚ùå Error: {e}\")\n        return\n    \n    # Create directories\n    output_dir = f'gradcam_{model_name.lower()}'\n    original_dir = f'{output_dir}/original'\n    overlay_dir = f'{output_dir}/overlay'\n    combined_dir = f'{output_dir}/combined'\n    \n    os.makedirs(original_dir, exist_ok=True)\n    os.makedirs(overlay_dir, exist_ok=True)\n    os.makedirs(combined_dir, exist_ok=True)\n    \n    # Generate visualizations\n    print(f\"\\nüî• Generating Grad-CAM...\")\n    \n    success_count = 0\n    error_count = 0\n    \n    for idx, (img, true_label) in enumerate(zip(images, labels)):\n        try:\n            # Prepare image\n            img_array = np.expand_dims(img, axis=0)\n            \n            # Get prediction\n            preds = model.predict(img_array, verbose=0)\n            pred_label = np.argmax(preds[0])\n            confidence = preds[0][pred_label]\n            \n            # Generate Grad-CAM\n            heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer, pred_index=pred_label)\n            gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n            \n            # Filename\n            base_filename = f'{idx+1:02d}_class{true_label}_pred{pred_label}'\n            \n            # Save original\n            fig_orig = plt.figure(figsize=(5, 5))\n            plt.imshow(img)\n            plt.axis('off')\n            plt.tight_layout(pad=0)\n            plt.savefig(f'{original_dir}/{base_filename}_original.png', \n                       dpi=150, bbox_inches='tight', pad_inches=0)\n            plt.close(fig_orig)\n            \n            # Save overlay\n            fig_overlay = plt.figure(figsize=(5, 5))\n            plt.imshow(gradcam_img)\n            plt.axis('off')\n            plt.tight_layout(pad=0)\n            plt.savefig(f'{overlay_dir}/{base_filename}_overlay.png', \n                       dpi=150, bbox_inches='tight', pad_inches=0)\n            plt.close(fig_overlay)\n            \n            # Combined visualization\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n            \n            axes[0].imshow(img)\n            axes[0].set_title(f'Original\\nTrue: {CLASS_LABELS[true_label]}', fontsize=10)\n            axes[0].axis('off')\n            \n            axes[1].imshow(heatmap, cmap='jet')\n            axes[1].set_title('Grad-CAM', fontsize=10)\n            axes[1].axis('off')\n            \n            axes[2].imshow(gradcam_img)\n            axes[2].set_title(f'Overlay\\nPred: {CLASS_LABELS[pred_label]} ({confidence:.2%})', fontsize=10)\n            axes[2].axis('off')\n            \n            correct = \"‚úì\" if true_label == pred_label else \"‚úó\"\n            fig.suptitle(f'{model_name} - Image {idx+1}/50 {correct}', \n                        fontsize=14, fontweight='bold')\n            \n            plt.tight_layout()\n            plt.savefig(f'{combined_dir}/{base_filename}_combined.png', \n                       dpi=150, bbox_inches='tight')\n            plt.close(fig)\n            \n            success_count += 1\n            \n            if (idx + 1) % 10 == 0:\n                print(f\"   ‚úì Processed {idx+1}/50 ({success_count} successful)\")\n                \n        except Exception as e:\n            error_count += 1\n            print(f\"   ‚ö†Ô∏è Error on image {idx+1}: {str(e)}\")\n            continue\n    \n    print(f\"\\n‚úÖ Completed: {success_count} successful, {error_count} errors\")\n    print(f\"   Output: '{output_dir}/'\")\n    \n    # Cleanup\n    del model\n    tf.keras.backend.clear_session()\n\n\n# Process all models\nfor model_name, model_path in available_models.items():\n    generate_gradcam_for_model(model_name, model_path, selected_images, selected_labels)\n\n\n# =============================================================================\n# Cell 5: Summary Comparison\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING SUMMARY COMPARISON\")\nprint(\"=\"*80)\n\ndef create_summary_comparison(models_dict, images, labels, num_samples=5):\n    \"\"\"\n    Create model comparison visualizations\n    \"\"\"\n    print(f\"\\nCreating comparison for {num_samples} samples...\")\n    \n    np.random.seed(42)\n    \n    # Select samples\n    sample_indices = []\n    for class_idx in range(min(5, num_samples)):\n        class_mask = labels == class_idx\n        class_imgs = np.where(class_mask)[0]\n        if len(class_imgs) > 0:\n            sample_indices.append(np.random.choice(class_imgs))\n    \n    # Load models\n    loaded_models = {}\n    conv_layers = {}\n    \n    print(\"Loading models...\")\n    for model_name, model_path in models_dict.items():\n        loaded_models[model_name] = load_model(model_path)\n        conv_layers[model_name] = get_last_conv_layer_name(loaded_models[model_name])\n        print(f\"   ‚úì {model_name}\")\n    \n    os.makedirs('gradcam_comparison', exist_ok=True)\n    \n    for sample_idx, img_idx in enumerate(sample_indices):\n        print(f\"   Comparison {sample_idx+1}/{len(sample_indices)}...\")\n        \n        img = images[img_idx]\n        true_label = labels[img_idx]\n        \n        num_models = len(loaded_models)\n        fig, axes = plt.subplots(1, num_models + 1, figsize=(5 * (num_models + 1), 5))\n        \n        # Original\n        axes[0].imshow(img)\n        axes[0].set_title(f'Original\\nTrue: {CLASS_LABELS[true_label]}', \n                         fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        \n        # Grad-CAM from each model\n        img_array = np.expand_dims(img, axis=0)\n        \n        for idx, (model_name, model) in enumerate(loaded_models.items()):\n            preds = model.predict(img_array, verbose=0)\n            pred_label = np.argmax(preds[0])\n            confidence = preds[0][pred_label]\n            \n            heatmap = make_gradcam_heatmap(img_array, model, conv_layers[model_name], pred_index=pred_label)\n            gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n            \n            axes[idx + 1].imshow(gradcam_img)\n            axes[idx + 1].set_title(f'{model_name}\\n{CLASS_LABELS[pred_label]} ({confidence:.1%})', \n                                   fontsize=11, fontweight='bold')\n            axes[idx + 1].axis('off')\n        \n        fig.suptitle(f'Model Comparison - Sample {sample_idx + 1} (Class: {CLASS_LABELS[true_label]})', \n                    fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        \n        plt.savefig(f'gradcam_comparison/comparison_sample_{sample_idx + 1}.png', \n                   dpi=150, bbox_inches='tight')\n        plt.close()\n    \n    print(f\"‚úÖ Saved to 'gradcam_comparison/'\")\n    \n    # Cleanup\n    for model in loaded_models.values():\n        del model\n    tf.keras.backend.clear_session()\n\n\n# Create comparison\nif len(available_models) > 1:\n    create_summary_comparison(available_models, selected_images, selected_labels, num_samples=5)\nelif len(available_models) == 1:\n    print(\"‚ö†Ô∏è  Only 1 model. Skipping comparison.\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ GRAD-CAM COMPLETED!\")\nprint(\"=\"*80)\nprint(f\"\\nüìÅ Output directories:\")\nfor model_name in available_models.keys():\n    model_dir = f\"gradcam_{model_name.lower()}\"\n    print(f\"   - {model_dir}/\")\nif len(available_models) > 1:\n    print(f\"   - gradcam_comparison/\")\nprint(\"\\nüéâ Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:15:26.403935Z","iopub.execute_input":"2025-10-22T08:15:26.404429Z","iopub.status.idle":"2025-10-22T08:16:54.721606Z","shell.execute_reply.started":"2025-10-22T08:15:26.404400Z","shell.execute_reply":"2025-10-22T08:16:54.720951Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nGRAD-CAM VISUALIZATION SCRIPT - FUNCTIONAL API VERSION\n================================================================================\n‚úÖ Class labels defined:\n   0: No DR\n   1: Mild\n   2: Moderate\n   3: Severe\n   4: Proliferative DR\n\nüìä Selecting 50 images (10 per class)...\n‚úÖ Selected 50 images\n   Class distribution: [10 10 10 10 10]\n‚ö†Ô∏è  Not found: EfficientNetV2M\n‚ö†Ô∏è  Not found: DenseNet169\n‚úÖ Found: InceptionV3\n‚ö†Ô∏è  Not found: EfficientNetB5\n\n‚úÖ Found 1 model(s)\n\n================================================================================\nProcessing: InceptionV3\n================================================================================\nLoading model...\n   ‚úì Model loaded\n   Model type: <class 'keras.src.models.sequential.Sequential'>\n   Number of layers: 6\n   Base model: <class 'keras.src.models.functional.Functional'>\n   Base model layers: 311\n‚úÖ Using layer: conv2d_93\n\nüî• Generating Grad-CAM...\n   ‚úì Processed 10/50 (10 successful)\n   ‚úì Processed 20/50 (20 successful)\n   ‚úì Processed 30/50 (30 successful)\n   ‚úì Processed 40/50 (40 successful)\n   ‚úì Processed 50/50 (50 successful)\n\n‚úÖ Completed: 50 successful, 0 errors\n   Output: 'gradcam_inceptionv3/'\n\n================================================================================\nCREATING SUMMARY COMPARISON\n================================================================================\n‚ö†Ô∏è  Only 1 model. Skipping comparison.\n\n================================================================================\n‚úÖ GRAD-CAM COMPLETED!\n================================================================================\n\nüìÅ Output directories:\n   - gradcam_inceptionv3/\n\nüéâ Done!\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}