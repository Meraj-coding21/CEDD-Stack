{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"},{"sourceId":12927902,"sourceType":"datasetVersion","datasetId":8180450},{"sourceId":12934671,"sourceType":"datasetVersion","datasetId":8185064}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =============================================================================\n# Cell 1: Reproducibility\n# =============================================================================\nSEED = 5\nimport os, random\nimport numpy as np\nimport tensorflow as tf\n\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:07:24.757223Z","iopub.execute_input":"2025-10-22T17:07:24.757453Z","iopub.status.idle":"2025-10-22T17:07:38.287058Z","shell.execute_reply.started":"2025-10-22T17:07:24.757429Z","shell.execute_reply":"2025-10-22T17:07:38.286216Z"}},"outputs":[{"name":"stderr","text":"2025-10-22 17:07:26.188433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761152846.369641      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761152846.426300      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# Cell 2: Core Imports and Constants\n# =============================================================================\nimport json, math, os, gc\nimport cv2\nfrom PIL import Image\nimport pickle\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import (\n    EfficientNetV2M, EfficientNetB5, InceptionV3, Xception, DenseNet169,\n    DenseNet121, InceptionResNetV2, ResNet50\n)\n# Machine Learning\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\nfrom sklearn.ensemble import VotingClassifier\nimport catboost as cb\nimport scipy\nfrom scipy.special import softmax\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n# XAI Libraries\nimport shap\nimport lime\nfrom lime import lime_image\nfrom lime.wrappers.scikit_image import SegmentationAlgorithm\nimport seaborn as sns\n# Jupyter\n%matplotlib inline\n\n# Constants\nIMG_SIZE = 224\nBATCH_SIZE = 24\nEPOCHS = 50\nPATIENCE = 7\n\n# GPU Memory Configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"‚úÖ GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"GPU configuration error: {e}\")\n\n# Enable XLA compilation\ntf.config.optimizer.set_jit(True)\nprint(\"‚úÖ XLA compilation enabled\")\n\n# Kaggle input paths\nAPTOS_DIR = \"/kaggle/input/aptos2019-blindness-detection\"\nPROCESSED_DATA_PATH = \"/kaggle/input/please/preprocessed_aptos_data.npz\"\nTEST_CSV_PATH = \"/kaggle/input/please/test_data.csv\"\nTRAIN_CSV_PATH = \"/kaggle/input/please/train_data.csv\"\n\n# Class labels\nCLASS_LABELS = ['No DR', 'Mild', 'Moderate', 'Severe', 'Proliferative DR']\n\nprint(\"‚úÖ All imports completed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:08:11.164347Z","iopub.execute_input":"2025-10-22T17:08:11.165110Z","iopub.status.idle":"2025-10-22T17:08:18.249363Z","shell.execute_reply.started":"2025-10-22T17:08:11.165082Z","shell.execute_reply":"2025-10-22T17:08:18.248751Z"}},"outputs":[{"name":"stdout","text":"‚úÖ GPU memory growth enabled for 1 GPU(s)\n‚úÖ XLA compilation enabled\n‚úÖ All imports completed successfully\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# Cell 3: Load Preprocessed Data - FIXED FOR EFFICIENTNET\n# =============================================================================\nprint(\"=\"*80)\nprint(\"LOADING DATA FOR EFFICIENTNET TRAINING\")\nprint(\"=\"*80)\n\n# Load preprocessed data\ndata = np.load(PROCESSED_DATA_PATH)\nprint(\"Available keys:\", list(data.keys()))\n\n# Load data\nx_train_full = data['X_train']\ny_train_full = data['y_train']\nx_test = data['X_test']  \ny_test = data['y_test']\n\n# Load CSV files\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ntest_df = pd.read_csv(TEST_CSV_PATH)\n\nprint(f\"\\nüìä Original data shapes:\")\nprint(f\"   Training data: {x_train_full.shape}\")\nprint(f\"   Training labels: {y_train_full.shape}\")\nprint(f\"   Test data: {x_test.shape}\")\nprint(f\"   Test labels: {y_test.shape}\")\n\n# CRITICAL: Check and convert labels to one-hot encoding\nif len(y_train_full.shape) == 1 or (len(y_train_full.shape) == 2 and y_train_full.shape[1] == 1):\n    print(\"\\nüîÑ Converting labels to one-hot encoding...\")\n    from tensorflow.keras.utils import to_categorical\n    y_train_full_onehot = to_categorical(y_train_full, num_classes=5)\n    y_test_onehot = to_categorical(y_test, num_classes=5)\nelse:\n    y_train_full_onehot = y_train_full\n    y_test_onehot = y_test\n\n# Store original integer labels for evaluation\nif len(y_train_full.shape) > 1:\n    y_train_original = np.argmax(y_train_full, axis=1)\n    y_test_original = np.argmax(y_test, axis=1)\nelse:\n    y_train_original = y_train_full.copy()\n    y_test_original = y_test.copy()\n\nprint(f\"\\n‚úÖ Label encoding completed:\")\nprint(f\"   One-hot shape: {y_train_full_onehot.shape}\")\nprint(f\"   Original integer labels saved for evaluation\")\n\n# CRITICAL FIX: Keep images in [0-255] range for EfficientNet\nprint(f\"\\nüîß EFFICIENTNET PREPROCESSING:\")\nprint(f\"   Current data range: [{x_train_full.min():.2f}, {x_train_full.max():.2f}]\")\n\nif x_train_full.max() <= 1.0:\n    # Data is normalized [0-1], convert back to [0-255]\n    print(\"   ‚ö†Ô∏è  Data is normalized. Converting to [0-255] for EfficientNet...\")\n    x_train_full = (x_train_full * 255.0).astype('float32')\n    x_test = (x_test * 255.0).astype('float32')\n    print(f\"   ‚úÖ Converted to range: [{x_train_full.min():.2f}, {x_train_full.max():.2f}]\")\nelif x_train_full.max() > 1.0:\n    print(\"   ‚úÖ Data already in [0-255] range\")\n    x_train_full = x_train_full.astype('float32')\n    x_test = x_test.astype('float32')\n\n# Verify class distribution\nprint(f\"\\nüìä Class distribution:\")\nprint(f\"   Training: {np.bincount(y_train_original)}\")\nprint(f\"   Test: {np.bincount(y_test_original)}\")\n\nprint(\"\\n‚úÖ Data loaded and prepared for EfficientNetB5 training\")\nprint(\"=\"*80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T17:08:43.062407Z","iopub.execute_input":"2025-10-22T17:08:43.063467Z","iopub.status.idle":"2025-10-22T17:09:00.179364Z","shell.execute_reply.started":"2025-10-22T17:08:43.063438Z","shell.execute_reply":"2025-10-22T17:09:00.178255Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nLOADING DATA FOR EFFICIENTNET TRAINING\n================================================================================\nAvailable keys: ['X_train', 'y_train', 'X_test', 'y_test']\n\nüìä Original data shapes:\n   Training data: (7220, 224, 224, 3)\n   Training labels: (7220, 5)\n   Test data: (733, 224, 224, 3)\n   Test labels: (733,)\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAxisError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1336918816.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0my_train_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0my_test_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0my_train_original\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[1;32m   1227\u001b[0m     \"\"\"\n\u001b[1;32m   1228\u001b[0m     \u001b[0mkwds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'keepdims'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NoValue\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# A TypeError occurs if the object does have such a method in its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAxisError\u001b[0m: axis 1 is out of bounds for array of dimension 1"],"ename":"AxisError","evalue":"axis 1 is out of bounds for array of dimension 1","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# Grad-CAM Visualization Script for Diabetic Retinopathy Models\n# Generates Grad-CAM heatmaps for 50 images (10 per class) across 4 models\n# Now saves original and overlay images separately\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"GRAD-CAM VISUALIZATION SCRIPT\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model, Model\nimport os\n\n# =============================================================================\n# Cell 1: Grad-CAM Implementation\n# =============================================================================\n\ndef get_last_conv_layer_name(model):\n    \"\"\"\n    Automatically find the last convolutional layer in the model\n    \"\"\"\n    # First check the main model layers\n    for layer in reversed(model.layers):\n        # Check if it's a convolutional layer by class name\n        if 'Conv' in layer.__class__.__name__:\n            return layer.name\n    \n    # If no conv layer found in main model, check inside base model (first layer)\n    if hasattr(model.layers[0], 'layers'):\n        for layer in reversed(model.layers[0].layers):\n            if 'Conv' in layer.__class__.__name__:\n                return layer.name\n    \n    raise ValueError(\"Could not find convolutional layer. Model might have unexpected architecture.\")\n\n\ndef make_gradcam_heatmap_simple(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"\n    Simplified Grad-CAM that works with Sequential models containing nested base models\n    \"\"\"\n    # Get base model\n    base_model = model.layers[0]\n    \n    # Get the target convolutional layer\n    target_layer = base_model.get_layer(last_conv_layer_name)\n    \n    # Create a sub-model from input to target conv layer\n    submodel = tf.keras.Model(inputs=base_model.input, outputs=target_layer.output)\n    \n    # Use GradientTape to compute gradients\n    with tf.GradientTape() as tape:\n        # Get conv layer activations\n        conv_output = submodel(img_array, training=False)\n        tape.watch(conv_output)\n        \n        # Continue forward pass through remaining layers\n        x = conv_output\n        \n        # Get all layers after target layer in base model\n        start_applying = False\n        for layer in base_model.layers:\n            if start_applying:\n                x = layer(x, training=False)\n            if layer.name == last_conv_layer_name:\n                start_applying = True\n        \n        # Apply remaining model layers (pooling, dense, etc.)\n        for layer in model.layers[1:]:\n            x = layer(x, training=False)\n        \n        preds = x\n        \n        # Get the predicted class\n        if pred_index is None:\n            pred_index = tf.argmax(preds[0])\n        \n        # Get the score for target class\n        target_class_output = preds[:, pred_index]\n    \n    # Compute gradients\n    grads = tape.gradient(target_class_output, conv_output)\n    \n    # Global average pooling on gradients\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    \n    # Weight channels by gradient importance and sum\n    conv_output = conv_output[0]\n    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_output), axis=-1)\n    \n    # Apply ReLU and normalize\n    heatmap = tf.nn.relu(heatmap)\n    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-10)\n    \n    return heatmap.numpy()\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"\n    Wrapper that calls the simplified version\n    \"\"\"\n    return make_gradcam_heatmap_simple(img_array, model, last_conv_layer_name, pred_index)\n\n\ndef overlay_gradcam(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    \"\"\"\n    Overlay Grad-CAM heatmap on original image\n    \n    Args:\n        img: Original image array (224, 224, 3) in range [0, 1]\n        heatmap: Grad-CAM heatmap\n        alpha: Transparency of heatmap overlay\n        colormap: OpenCV colormap to use\n    \n    Returns:\n        superimposed_img: Image with Grad-CAM overlay\n    \"\"\"\n    # Resize heatmap to match image size\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    \n    # Convert heatmap to RGB\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    \n    # Convert image to uint8\n    img_uint8 = np.uint8(255 * img)\n    \n    # Superimpose the heatmap on original image\n    superimposed_img = heatmap * alpha + img_uint8\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n    \n    return superimposed_img\n\n\n# =============================================================================\n# Cell 2: Select 50 Images (10 per class)\n# =============================================================================\n\ndef select_images_per_class(x_data, y_data, num_per_class=10, random_seed=42):\n    \"\"\"\n    Select specified number of images per class with fixed random seed\n    \n    Args:\n        x_data: Image data\n        y_data: Labels (one-hot encoded or integer)\n        num_per_class: Number of images to select per class\n        random_seed: Random seed for reproducibility\n    \n    Returns:\n        selected_indices: Indices of selected images\n        selected_images: Selected image arrays\n        selected_labels: Selected labels\n    \"\"\"\n    # Set random seed for reproducibility\n    np.random.seed(random_seed)\n    \n    # Convert one-hot to integer labels if needed\n    if len(y_data.shape) > 1:\n        labels = np.argmax(y_data, axis=1)\n    else:\n        labels = y_data\n    \n    selected_indices = []\n    \n    # Select images for each class\n    for class_idx in range(5):\n        # Get indices of all images for this class\n        class_indices = np.where(labels == class_idx)[0]\n        \n        # Randomly select num_per_class images\n        if len(class_indices) >= num_per_class:\n            selected = np.random.choice(class_indices, num_per_class, replace=False)\n        else:\n            # If not enough images, select all available\n            selected = class_indices\n            print(f\"Warning: Only {len(class_indices)} images available for class {class_idx}\")\n        \n        selected_indices.extend(selected)\n    \n    selected_indices = np.array(selected_indices)\n    selected_images = x_data[selected_indices]\n    selected_labels = labels[selected_indices]\n    \n    return selected_indices, selected_images, selected_labels\n\n\nprint(\"\\nüìä Selecting 50 images (10 per class) with fixed random seed...\")\nselected_indices, selected_images, selected_labels = select_images_per_class(\n    x_test, y_test, num_per_class=10, random_seed=42\n)\n\nprint(f\"‚úÖ Selected {len(selected_indices)} images\")\nprint(f\"   Class distribution: {np.bincount(selected_labels)}\")\n\n\n# =============================================================================\n# Cell 3: Load Models and Generate Grad-CAM\n# =============================================================================\n\n# Model paths - UPDATE THESE PATHS BASED ON YOUR SAVED MODELS\nMODEL_PATHS = {\n    'EfficientNetV2M': '/kaggle/working/effnetv2m_final.h5',\n    'DenseNet169': '/kaggle/working/densenet169_best.h5',\n    'InceptionV3': '/kaggle/working/inceptionv3_final.h5',\n    'EfficientNetB5': '/kaggle/working/effnetb5_final.h5'\n}\n\n# Check which models are available\navailable_models = {}\nfor model_name, model_path in MODEL_PATHS.items():\n    if os.path.exists(model_path):\n        available_models[model_name] = model_path\n        print(f\"‚úÖ Found: {model_name}\")\n    else:\n        print(f\"‚ö†Ô∏è  Not found: {model_name} at {model_path}\")\n\nif len(available_models) == 0:\n    print(\"\\n‚ùå No models found! Please update MODEL_PATHS with correct paths.\")\n    print(\"   Current directory contents:\")\n    print(\"  \", os.listdir('.'))\nelse:\n    print(f\"\\n‚úÖ Found {len(available_models)} model(s) to process\")\n\n\n# =============================================================================\n# Cell 4: Generate Grad-CAM for All Models\n# =============================================================================\n\ndef generate_gradcam_for_model(model_name, model_path, images, labels):\n    \"\"\"\n    Generate Grad-CAM visualizations for a specific model\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Processing: {model_name}\")\n    print(f\"{'='*80}\")\n    \n    # Load model\n    print(f\"Loading model from {model_path}...\")\n    model = load_model(model_path)\n    \n    # Find last convolutional layer\n    try:\n        last_conv_layer = get_last_conv_layer_name(model)\n        print(f\"‚úÖ Using convolutional layer: {last_conv_layer}\")\n    except ValueError as e:\n        print(f\"‚ùå Error: {e}\")\n        return\n    \n    # Create output directories\n    output_dir = f'gradcam_{model_name.lower()}'\n    original_dir = f'{output_dir}/original'\n    overlay_dir = f'{output_dir}/overlay'\n    combined_dir = f'{output_dir}/combined'\n    \n    os.makedirs(original_dir, exist_ok=True)\n    os.makedirs(overlay_dir, exist_ok=True)\n    os.makedirs(combined_dir, exist_ok=True)\n    \n    # Generate Grad-CAM for each image\n    print(f\"\\nüî• Generating Grad-CAM visualizations...\")\n    \n    for idx, (img, true_label) in enumerate(zip(images, labels)):\n        # Prepare image for model\n        img_array = np.expand_dims(img, axis=0)\n        \n        # Get prediction\n        preds = model.predict(img_array, verbose=0)\n        pred_label = np.argmax(preds[0])\n        confidence = preds[0][pred_label]\n        \n        # Generate Grad-CAM heatmap\n        heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer, pred_index=pred_label)\n        \n        # Create overlay\n        gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n        \n        # Base filename\n        base_filename = f'{idx+1:02d}_class{true_label}_pred{pred_label}'\n        \n        # Save original image separately\n        fig_orig = plt.figure(figsize=(5, 5))\n        plt.imshow(img)\n        plt.axis('off')\n        plt.tight_layout(pad=0)\n        plt.savefig(f'{original_dir}/{base_filename}_original.png', \n                   dpi=150, bbox_inches='tight', pad_inches=0)\n        plt.close(fig_orig)\n        \n        # Save overlay image separately\n        fig_overlay = plt.figure(figsize=(5, 5))\n        plt.imshow(gradcam_img)\n        plt.axis('off')\n        plt.tight_layout(pad=0)\n        plt.savefig(f'{overlay_dir}/{base_filename}_overlay.png', \n                   dpi=150, bbox_inches='tight', pad_inches=0)\n        plt.close(fig_overlay)\n        \n        # Create and save combined visualization\n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n        \n        # Original image\n        axes[0].imshow(img)\n        axes[0].set_title(f'Original Image\\nTrue: {CLASS_LABELS[true_label]}', fontsize=10)\n        axes[0].axis('off')\n        \n        # Heatmap only\n        axes[1].imshow(heatmap, cmap='jet')\n        axes[1].set_title('Grad-CAM Heatmap', fontsize=10)\n        axes[1].axis('off')\n        \n        # Overlay\n        axes[2].imshow(gradcam_img)\n        axes[2].set_title(f'Grad-CAM Overlay\\nPred: {CLASS_LABELS[pred_label]} ({confidence:.2%})', fontsize=10)\n        axes[2].axis('off')\n        \n        # Add main title\n        correct = \"‚úì\" if true_label == pred_label else \"‚úó\"\n        fig.suptitle(f'{model_name} - Image {idx+1}/50 {correct}', \n                     fontsize=14, fontweight='bold')\n        \n        plt.tight_layout()\n        \n        # Save combined figure\n        plt.savefig(f'{combined_dir}/{base_filename}_combined.png', \n                   dpi=150, bbox_inches='tight')\n        plt.close(fig)\n        \n        if (idx + 1) % 10 == 0:\n            print(f\"   Processed {idx+1}/50 images...\")\n    \n    print(f\"‚úÖ All visualizations saved to '{output_dir}/' directory\")\n    print(f\"   - Original images: {original_dir}/\")\n    print(f\"   - Overlay images: {overlay_dir}/\")\n    print(f\"   - Combined views: {combined_dir}/\")\n    \n    # Clean up\n    del model\n    tf.keras.backend.clear_session()\n\n\n# Process all available models\nfor model_name, model_path in available_models.items():\n    generate_gradcam_for_model(model_name, model_path, selected_images, selected_labels)\n\n\n# =============================================================================\n# Cell 5: Create Summary Visualization\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING SUMMARY COMPARISON\")\nprint(\"=\"*80)\n\ndef create_summary_comparison(models_dict, images, labels, num_samples=5):\n    \"\"\"\n    Create a summary comparison showing Grad-CAM from all models side-by-side\n    \"\"\"\n    print(f\"\\nCreating summary comparison for {num_samples} sample images...\")\n    \n    # Set random seed for reproducibility\n    np.random.seed(42)\n    \n    # Randomly select sample images (one from each class)\n    sample_indices = []\n    for class_idx in range(min(5, num_samples)):\n        class_mask = labels == class_idx\n        class_imgs = np.where(class_mask)[0]\n        if len(class_imgs) > 0:\n            sample_indices.append(np.random.choice(class_imgs))\n    \n    # Load all models\n    loaded_models = {}\n    conv_layers = {}\n    \n    for model_name, model_path in models_dict.items():\n        loaded_models[model_name] = load_model(model_path)\n        conv_layers[model_name] = get_last_conv_layer_name(loaded_models[model_name])\n    \n    # Create comparison for each sample\n    os.makedirs('gradcam_comparison', exist_ok=True)\n    \n    for sample_idx, img_idx in enumerate(sample_indices):\n        img = images[img_idx]\n        true_label = labels[img_idx]\n        \n        num_models = len(loaded_models)\n        fig, axes = plt.subplots(1, num_models + 1, figsize=(5 * (num_models + 1), 5))\n        \n        # Original image\n        axes[0].imshow(img)\n        axes[0].set_title(f'Original\\nTrue: {CLASS_LABELS[true_label]}', \n                         fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        \n        # Grad-CAM from each model\n        img_array = np.expand_dims(img, axis=0)\n        \n        for idx, (model_name, model) in enumerate(loaded_models.items()):\n            # Get prediction\n            preds = model.predict(img_array, verbose=0)\n            pred_label = np.argmax(preds[0])\n            confidence = preds[0][pred_label]\n            \n            # Generate Grad-CAM\n            heatmap = make_gradcam_heatmap(img_array, model, conv_layers[model_name], pred_index=pred_label)\n            gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n            \n            # Plot\n            axes[idx + 1].imshow(gradcam_img)\n            axes[idx + 1].set_title(f'{model_name}\\n{CLASS_LABELS[pred_label]} ({confidence:.1%})', \n                                   fontsize=11, fontweight='bold')\n            axes[idx + 1].axis('off')\n        \n        fig.suptitle(f'Model Comparison - Sample {sample_idx + 1} (Class: {CLASS_LABELS[true_label]})', \n                    fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        \n        # Save\n        plt.savefig(f'gradcam_comparison/comparison_sample_{sample_idx + 1}.png', \n                   dpi=150, bbox_inches='tight')\n        plt.close()\n    \n    print(f\"‚úÖ Summary comparisons saved to 'gradcam_comparison/' directory\")\n    \n    # Clean up\n    for model in loaded_models.values():\n        del model\n    tf.keras.backend.clear_session()\n\n\n# Create summary comparison if multiple models available\nif len(available_models) > 1:\n    create_summary_comparison(available_models, selected_images, selected_labels, num_samples=5)\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"‚úÖ GRAD-CAM VISUALIZATION COMPLETED!\")\nprint(\"=\"*80)\nprint(f\"\\nüìÅ Output directories created:\")\nfor model_name in available_models.keys():\n    model_dir = f\"gradcam_{model_name.lower()}\"\n    print(f\"   - {model_dir}/original/  (50 original images)\")\n    print(f\"   - {model_dir}/overlay/   (50 overlay images)\")\n    print(f\"   - {model_dir}/combined/  (50 combined visualizations)\")\nif len(available_models) > 1:\n    print(f\"   - gradcam_comparison/  (5 comparison images)\")\nprint(\"\\nüéâ All Grad-CAM visualizations generated successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T07:30:35.524954Z","iopub.execute_input":"2025-10-22T07:30:35.525661Z","iopub.status.idle":"2025-10-22T07:33:04.467631Z","shell.execute_reply.started":"2025-10-22T07:30:35.525635Z","shell.execute_reply":"2025-10-22T07:33:04.466891Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nGRAD-CAM VISUALIZATION SCRIPT\n================================================================================\n\nüìä Selecting 50 images (10 per class) with fixed random seed...\n‚úÖ Selected 50 images\n   Class distribution: [10 10 10 10 10]\n‚úÖ Found: EfficientNetV2M\n‚ö†Ô∏è  Not found: DenseNet169 at /kaggle/working/densenet169_best.h5\n‚ö†Ô∏è  Not found: InceptionV3 at /kaggle/working/inceptionv3_final.h5\n‚ö†Ô∏è  Not found: EfficientNetB5 at /kaggle/working/effnetb5_final.h5\n\n‚úÖ Found 1 model(s) to process\n\n================================================================================\nProcessing: EfficientNetV2M\n================================================================================\nLoading model from /kaggle/working/effnetv2m_final.h5...\n‚úÖ Using convolutional layer: top_conv\n\nüî• Generating Grad-CAM visualizations...\n   Processed 10/50 images...\n   Processed 20/50 images...\n   Processed 30/50 images...\n   Processed 40/50 images...\n   Processed 50/50 images...\n‚úÖ All visualizations saved to 'gradcam_efficientnetv2m/' directory\n   - Original images: gradcam_efficientnetv2m/original/\n   - Overlay images: gradcam_efficientnetv2m/overlay/\n   - Combined views: gradcam_efficientnetv2m/combined/\n\n================================================================================\nCREATING SUMMARY COMPARISON\n================================================================================\n\n================================================================================\n‚úÖ GRAD-CAM VISUALIZATION COMPLETED!\n================================================================================\n\nüìÅ Output directories created:\n   - gradcam_efficientnetv2m/original/  (50 original images)\n   - gradcam_efficientnetv2m/overlay/   (50 overlay images)\n   - gradcam_efficientnetv2m/combined/  (50 combined visualizations)\n\nüéâ All Grad-CAM visualizations generated successfully!\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}