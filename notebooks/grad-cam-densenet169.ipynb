{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14774,"databundleVersionId":875431,"sourceType":"competition"},{"sourceId":12827129,"sourceType":"datasetVersion","datasetId":8112000}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Cell 1: Reproducibility Setup\n# =============================================================================\nSEED = 5\nimport os, random\nimport numpy as np\nimport tensorflow as tf\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:11:39.286530Z","iopub.execute_input":"2025-09-03T07:11:39.286766Z","iopub.status.idle":"2025-09-03T07:11:52.640776Z","shell.execute_reply.started":"2025-09-03T07:11:39.286741Z","shell.execute_reply":"2025-09-03T07:11:52.639925Z"}},"outputs":[{"name":"stderr","text":"2025-09-03 07:11:40.699611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756883500.872422      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756883500.920529      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# =============================================================================\n# Cell 2: Core Imports and Constants\n# =============================================================================\nimport json, math, os, gc\nimport cv2\nfrom PIL import Image\nimport pickle\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import DenseNet169\nfrom tensorflow.keras.utils import to_categorical\n\n# Machine Learning\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report\n)\n\n# XAI Libraries\nimport lime\nfrom lime import lime_image\nfrom lime.wrappers.scikit_image import SegmentationAlgorithm\nimport seaborn as sns\nfrom skimage.segmentation import mark_boundaries\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Constants\nIMG_SIZE = 224\nBATCH_SIZE = 24\nEPOCHS = 50\nPATIENCE = 7\n\n# GPU Memory Configuration\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        print(f\"âœ… GPU memory growth enabled for {len(gpus)} GPU(s)\")\n    except RuntimeError as e:\n        print(f\"GPU configuration error: {e}\")\n\n# Enable XLA compilation for better memory efficiency\ntf.config.optimizer.set_jit(True)\nprint(\"âœ… XLA compilation enabled for memory efficiency\")\n\n# Kaggle input paths\nPROCESSED_DATA_PATH = \"/kaggle/input/please/preprocessed_aptos_data.npz\"\nTEST_CSV_PATH = \"/kaggle/input/please/test_data.csv\"\nTRAIN_CSV_PATH = \"/kaggle/input/please/train_data.csv\"\n\nprint(\"âœ… All imports completed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:12:08.139990Z","iopub.execute_input":"2025-09-03T07:12:08.140693Z","iopub.status.idle":"2025-09-03T07:12:09.630303Z","shell.execute_reply.started":"2025-09-03T07:12:08.140669Z","shell.execute_reply":"2025-09-03T07:12:09.629607Z"}},"outputs":[{"name":"stdout","text":"âœ… GPU memory growth enabled for 1 GPU(s)\nâœ… XLA compilation enabled for memory efficiency\nâœ… All imports completed successfully\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# =============================================================================\n# Cell 3: Load and Prepare Data\n# =============================================================================\nprint(\"Loading preprocessed data...\")\n\n# Load preprocessed data\ndata = np.load(PROCESSED_DATA_PATH)\nprint(\"Available keys in preprocessed data:\", list(data.keys()))\n\n# Load data using the correct keys\nx_train_full = data['X_train']\ny_train_full = data['y_train']\nx_test = data['X_test']  \ny_test = data['y_test']\n\n# Load CSV files for additional metadata\ntrain_df = pd.read_csv(TRAIN_CSV_PATH)\ntest_df = pd.read_csv(TEST_CSV_PATH)\n\nprint(f\"Training data shape: {x_train_full.shape}\")\nprint(f\"Training labels shape: {y_train_full.shape}\")\nprint(f\"Test data shape: {x_test.shape}\")\nprint(f\"Test labels shape: {y_test.shape}\")\n\n# Convert labels to one-hot encoding if needed\nif len(y_train_full.shape) == 1 or (len(y_train_full.shape) == 2 and y_train_full.shape[1] == 1):\n    print(\"Converting training labels to one-hot encoding...\")\n    y_train_full = to_categorical(y_train_full, num_classes=5)\n    print(f\"New training labels shape: {y_train_full.shape}\")\n\nif len(y_test.shape) == 1 or (len(y_test.shape) == 2 and y_test.shape[1] == 1):\n    print(\"Converting test labels to one-hot encoding...\")\n    y_test = to_categorical(y_test, num_classes=5)\n    print(f\"New test labels shape: {y_test.shape}\")\n\n# Normalize pixel values to [0,1] if needed\nif x_train_full.max() > 1.0:\n    print(\"Normalizing pixel values to [0,1]...\")\n    x_train_full = x_train_full.astype('float32') / 255.0\n    x_test = x_test.astype('float32') / 255.0\nelse:\n    print(\"Data already normalized\")\n    x_train_full = x_train_full.astype('float32')\n    x_test = x_test.astype('float32')\n\n# Split training data into train/validation (80/20 split for single fold)\nx_train, x_val, y_train, y_val = train_test_split(\n    x_train_full, y_train_full, \n    test_size=0.2, \n    random_state=SEED, \n    stratify=np.argmax(y_train_full, axis=1)\n)\n\nprint(f\"Final data splits:\")\nprint(f\"  Training: {x_train.shape}\")\nprint(f\"  Validation: {x_val.shape}\")\nprint(f\"  Test: {x_test.shape}\")\n\nprint(\"âœ… Data loaded and prepared successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:12:13.421809Z","iopub.execute_input":"2025-09-03T07:12:13.422313Z","iopub.status.idle":"2025-09-03T07:12:32.882042Z","shell.execute_reply.started":"2025-09-03T07:12:13.422288Z","shell.execute_reply":"2025-09-03T07:12:32.881433Z"}},"outputs":[{"name":"stdout","text":"Loading preprocessed data...\nAvailable keys in preprocessed data: ['X_train', 'y_train', 'X_test', 'y_test']\nTraining data shape: (7220, 224, 224, 3)\nTraining labels shape: (7220, 5)\nTest data shape: (733, 224, 224, 3)\nTest labels shape: (733,)\nConverting test labels to one-hot encoding...\nNew test labels shape: (733, 5)\nNormalizing pixel values to [0,1]...\nFinal data splits:\n  Training: (5776, 224, 224, 3)\n  Validation: (1444, 224, 224, 3)\n  Test: (733, 224, 224, 3)\nâœ… Data loaded and prepared successfully\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# =============================================================================\n# Cell 4: MixupGenerator Implementation\n# =============================================================================\nclass MixupGenerator:\n    def __init__(self, X_train, y_train, batch_size=32, alpha=0.2, shuffle=True, datagen=None):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.batch_size = batch_size\n        self.alpha = alpha\n        self.shuffle = shuffle\n        self.sample_num = len(X_train)\n        self.datagen = datagen\n        \n    def __call__(self):\n        while True:\n            indexes = self.__get_exploration_order()\n            itr_num = int(len(indexes) // (self.batch_size * 2))\n            for i in range(itr_num):\n                batch_ids = indexes[i * self.batch_size * 2: (i + 1) * self.batch_size * 2]\n                X, y = self.__data_generation(batch_ids)\n                yield X, y\n                \n    def __get_exploration_order(self):\n        indexes = np.arange(self.sample_num)\n        if self.shuffle:\n            np.random.shuffle(indexes)\n        return indexes\n    \n    def __data_generation(self, batch_ids):\n        _, h, w, c = self.X_train.shape\n        l = np.random.beta(self.alpha, self.alpha, self.batch_size)\n        X_l = l.reshape(self.batch_size, 1, 1, 1)\n        y_l = l.reshape(self.batch_size, 1)\n        \n        X1 = self.X_train[batch_ids[:self.batch_size]]\n        X2 = self.X_train[batch_ids[self.batch_size:]]\n        X = X1 * X_l + X2 * (1 - X_l)\n        \n        if self.datagen:\n            for i in range(self.batch_size):\n                X[i] = self.datagen.random_transform(X[i])\n                X[i] = self.datagen.standardize(X[i])\n        \n        y1 = self.y_train[batch_ids[:self.batch_size]]\n        y2 = self.y_train[batch_ids[self.batch_size:]]\n        y = y1 * y_l + y2 * (1 - y_l)\n        \n        return X, y\n\ndef create_datagen():\n    return ImageDataGenerator(\n        zoom_range=0.15,\n        fill_mode='constant',\n        cval=0.,\n        horizontal_flip=True,\n        vertical_flip=True,\n    )\n\nprint(\"âœ… MixupGenerator defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:12:36.556986Z","iopub.execute_input":"2025-09-03T07:12:36.557636Z","iopub.status.idle":"2025-09-03T07:12:36.566324Z","shell.execute_reply.started":"2025-09-03T07:12:36.557613Z","shell.execute_reply":"2025-09-03T07:12:36.565538Z"}},"outputs":[{"name":"stdout","text":"âœ… MixupGenerator defined\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Cell 5: DenseNet Architecture Definition\n# =============================================================================\ndef build_densenet169():\n    \"\"\"\n    Build DenseNet169 model with the same architecture pattern as other models\n    \"\"\"\n    base_model = DenseNet169(\n        include_top=False,\n        weights='imagenet',\n        input_shape=(IMG_SIZE, IMG_SIZE, 3)\n    )\n    \n    model = Sequential([\n        base_model,\n        layers.GlobalAveragePooling2D(),\n        layers.Dropout(0.5),\n        layers.Dense(1024, activation='relu'),\n        layers.Dropout(0.5),\n        layers.Dense(5, activation='softmax')\n    ])\n    \n    # Make all layers trainable\n    for layer in model.layers:\n        layer.trainable = True\n    \n    model.compile(\n        loss='categorical_crossentropy',\n        optimizer=Adam(learning_rate=0.00005),\n        metrics=['accuracy']\n    )\n    \n    return model\n\nprint(\"âœ… DenseNet169 architecture defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:12:40.213236Z","iopub.execute_input":"2025-09-03T07:12:40.213846Z","iopub.status.idle":"2025-09-03T07:12:40.219944Z","shell.execute_reply.started":"2025-09-03T07:12:40.213809Z","shell.execute_reply":"2025-09-03T07:12:40.219098Z"}},"outputs":[{"name":"stdout","text":"âœ… DenseNet169 architecture defined\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Cell 6: Single Fold Training Function\n# =============================================================================\ndef train_single_fold_model(model_name, model_fn, x_train, y_train, x_val, y_val, x_test, y_test):\n    \"\"\"\n    Train a single model without cross-validation\n    \"\"\"\n    print(f\"\\n{'='*60}\")\n    print(f\"Training {model_name} - Single Fold\")\n    print(f\"{'='*60}\")\n    \n    # Clear session and collect garbage\n    tf.keras.backend.clear_session()\n    gc.collect()\n    \n    print(f\"Training samples: {len(x_train)}\")\n    print(f\"Validation samples: {len(x_val)}\")\n    print(f\"Test samples: {len(x_test)}\")\n    \n    # Build model\n    model = model_fn()\n    print(f\"Model built with {model.count_params():,} parameters\")\n    \n    # Setup callbacks\n    callbacks = [\n        EarlyStopping(\n            monitor='val_accuracy',\n            patience=PATIENCE,\n            restore_best_weights=True,\n            verbose=1\n        ),\n        ReduceLROnPlateau(\n            monitor='val_accuracy',\n            factor=0.5,\n            patience=3,\n            min_lr=1e-8,\n            verbose=1\n        )\n    ]\n    \n    # Setup data generators\n    datagen = create_datagen()\n    mixup_gen = MixupGenerator(x_train, y_train, batch_size=BATCH_SIZE, datagen=datagen)\n    \n    # Calculate steps per epoch\n    steps_per_epoch = len(x_train) // (BATCH_SIZE * 2)\n    print(f\"Steps per epoch: {steps_per_epoch}, Batch size: {BATCH_SIZE}\")\n    \n    # Train model\n    try:\n        history = model.fit(\n            mixup_gen(),\n            steps_per_epoch=steps_per_epoch,\n            epochs=EPOCHS,\n            validation_data=(x_val, y_val),\n            validation_batch_size=BATCH_SIZE,\n            callbacks=callbacks,\n            verbose=1\n        )\n    except tf.errors.ResourceExhaustedError:\n        print(\"âš ï¸ OOM error, trying with smaller batch size...\")\n        smaller_batch = BATCH_SIZE // 2\n        mixup_gen_small = MixupGenerator(x_train, y_train, batch_size=smaller_batch, datagen=datagen)\n        steps_per_epoch_small = len(x_train) // (smaller_batch * 2)\n        \n        history = model.fit(\n            mixup_gen_small(),\n            steps_per_epoch=steps_per_epoch_small,\n            epochs=EPOCHS,\n            validation_data=(x_val, y_val),\n            validation_batch_size=smaller_batch,\n            callbacks=callbacks,\n            verbose=1\n        )\n        print(f\"âœ… Training completed with reduced batch size: {smaller_batch}\")\n    \n    # Evaluate on validation and test sets\n    val_loss, val_acc = model.evaluate(x_val, y_val, batch_size=BATCH_SIZE, verbose=0)\n    test_loss, test_acc = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE, verbose=0)\n    \n    # Get predictions for detailed metrics\n    y_val_pred = model.predict(x_val, batch_size=BATCH_SIZE, verbose=0)\n    y_test_pred = model.predict(x_test, batch_size=BATCH_SIZE, verbose=0)\n    \n    # Convert predictions and true labels\n    y_val_true_labels = np.argmax(y_val, axis=1)\n    y_val_pred_labels = np.argmax(y_val_pred, axis=1)\n    y_test_true_labels = np.argmax(y_test, axis=1)\n    y_test_pred_labels = np.argmax(y_test_pred, axis=1)\n    \n    # Calculate detailed metrics\n    val_f1 = f1_score(y_val_true_labels, y_val_pred_labels, average='weighted')\n    val_precision = precision_score(y_val_true_labels, y_val_pred_labels, average='weighted', zero_division=0)\n    val_recall = recall_score(y_val_true_labels, y_val_pred_labels, average='weighted')\n    \n    test_f1 = f1_score(y_test_true_labels, y_test_pred_labels, average='weighted')\n    test_precision = precision_score(y_test_true_labels, y_test_pred_labels, average='weighted', zero_division=0)\n    test_recall = recall_score(y_test_true_labels, y_test_pred_labels, average='weighted')\n    \n    # Print results\n    print(f\"\\n{model_name} Training Results:\")\n    print(f\"{'='*50}\")\n    print(f\"Validation Metrics:\")\n    print(f\"  Accuracy:  {val_acc:.4f}\")\n    print(f\"  F1 Score:  {val_f1:.4f}\")\n    print(f\"  Precision: {val_precision:.4f}\")\n    print(f\"  Recall:    {val_recall:.4f}\")\n    print(f\"Test Metrics:\")\n    print(f\"  Accuracy:  {test_acc:.4f}\")\n    print(f\"  F1 Score:  {test_f1:.4f}\")\n    print(f\"  Precision: {test_precision:.4f}\")\n    print(f\"  Recall:    {test_recall:.4f}\")\n    \n    metrics = {\n        'val_accuracy': val_acc,\n        'val_f1': val_f1,\n        'val_precision': val_precision,\n        'val_recall': val_recall,\n        'test_accuracy': test_acc,\n        'test_f1': test_f1,\n        'test_precision': test_precision,\n        'test_recall': test_recall,\n        'val_predictions': y_val_pred,\n        'test_predictions': y_test_pred,\n        'val_true_labels': y_val_true_labels,\n        'test_true_labels': y_test_true_labels\n    }\n    \n    return model, history, metrics\n\nprint(\"âœ… Single fold training function defined\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:13:27.169441Z","iopub.execute_input":"2025-09-03T07:13:27.170197Z","iopub.status.idle":"2025-09-03T07:13:27.182646Z","shell.execute_reply.started":"2025-09-03T07:13:27.170173Z","shell.execute_reply":"2025-09-03T07:13:27.181837Z"}},"outputs":[{"name":"stdout","text":"âœ… Single fold training function defined\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# =============================================================================\n# Cell 7: Train DenseNet169 Model\n# =============================================================================\nprint(\"Starting DenseNet169 training...\")\n\n# Train the model\nmodel, history, metrics = train_single_fold_model(\n    'DenseNet169', build_densenet169,\n    x_train, y_train, x_val, y_val, x_test, y_test\n)\n\nprint(\"âœ… DenseNet169 training completed successfully\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-03T07:13:30.494961Z","iopub.execute_input":"2025-09-03T07:13:30.495212Z","iopub.status.idle":"2025-09-03T07:44:09.874350Z","shell.execute_reply.started":"2025-09-03T07:13:30.495196Z","shell.execute_reply":"2025-09-03T07:44:09.873562Z"}},"outputs":[{"name":"stdout","text":"Starting DenseNet169 training...\n\n============================================================\nTraining DenseNet169 - Single Fold\n============================================================\nTraining samples: 5776\nValidation samples: 1444\nTest samples: 733\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1756883611.100232      36 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/densenet/densenet169_weights_tf_dim_ordering_tf_kernels_notop.h5\n\u001b[1m51877672/51877672\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\nModel built with 14,352,965 parameters\nSteps per epoch: 120, Batch size: 24\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1756883687.179056      98 service.cc:148] XLA service 0x7c3b98003e70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1756883687.179896      98 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1756883687.212695      98 cuda_dnn.cc:529] Loaded cuDNN version 90300\nI0000 00:00:1756883687.326312      98 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m352s\u001b[0m 557ms/step - accuracy: 0.3167 - loss: 1.7668 - val_accuracy: 0.5048 - val_loss: 1.1566 - learning_rate: 5.0000e-05\nEpoch 2/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 273ms/step - accuracy: 0.5238 - loss: 1.2863 - val_accuracy: 0.5727 - val_loss: 1.0551 - learning_rate: 5.0000e-05\nEpoch 3/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 269ms/step - accuracy: 0.5814 - loss: 1.1467 - val_accuracy: 0.6766 - val_loss: 0.8277 - learning_rate: 5.0000e-05\nEpoch 4/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.6133 - loss: 1.0674 - val_accuracy: 0.7348 - val_loss: 0.6544 - learning_rate: 5.0000e-05\nEpoch 5/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 256ms/step - accuracy: 0.6501 - loss: 0.9971 - val_accuracy: 0.7618 - val_loss: 0.5826 - learning_rate: 5.0000e-05\nEpoch 6/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 251ms/step - accuracy: 0.6906 - loss: 0.9315 - val_accuracy: 0.7632 - val_loss: 0.6111 - learning_rate: 5.0000e-05\nEpoch 7/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.7034 - loss: 0.8927 - val_accuracy: 0.7992 - val_loss: 0.5312 - learning_rate: 5.0000e-05\nEpoch 8/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 252ms/step - accuracy: 0.7685 - loss: 0.8052 - val_accuracy: 0.8116 - val_loss: 0.5253 - learning_rate: 5.0000e-05\nEpoch 9/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 248ms/step - accuracy: 0.7718 - loss: 0.7603 - val_accuracy: 0.8089 - val_loss: 0.6000 - learning_rate: 5.0000e-05\nEpoch 10/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 253ms/step - accuracy: 0.7725 - loss: 0.7382 - val_accuracy: 0.8373 - val_loss: 0.4747 - learning_rate: 5.0000e-05\nEpoch 11/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 253ms/step - accuracy: 0.8136 - loss: 0.6720 - val_accuracy: 0.8442 - val_loss: 0.4352 - learning_rate: 5.0000e-05\nEpoch 12/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 252ms/step - accuracy: 0.8272 - loss: 0.6806 - val_accuracy: 0.8539 - val_loss: 0.4289 - learning_rate: 5.0000e-05\nEpoch 13/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 255ms/step - accuracy: 0.8346 - loss: 0.6417 - val_accuracy: 0.8968 - val_loss: 0.3324 - learning_rate: 5.0000e-05\nEpoch 14/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 248ms/step - accuracy: 0.8492 - loss: 0.6016 - val_accuracy: 0.8663 - val_loss: 0.4174 - learning_rate: 5.0000e-05\nEpoch 15/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 256ms/step - accuracy: 0.8788 - loss: 0.5549 - val_accuracy: 0.9224 - val_loss: 0.2886 - learning_rate: 5.0000e-05\nEpoch 16/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.8770 - loss: 0.5409 - val_accuracy: 0.9003 - val_loss: 0.3153 - learning_rate: 5.0000e-05\nEpoch 17/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.8757 - loss: 0.5672 - val_accuracy: 0.9148 - val_loss: 0.2653 - learning_rate: 5.0000e-05\nEpoch 18/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 224ms/step - accuracy: 0.8753 - loss: 0.5533\nEpoch 18: ReduceLROnPlateau reducing learning rate to 2.499999936844688e-05.\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.8753 - loss: 0.5532 - val_accuracy: 0.8885 - val_loss: 0.3570 - learning_rate: 5.0000e-05\nEpoch 19/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 255ms/step - accuracy: 0.8909 - loss: 0.5277 - val_accuracy: 0.9349 - val_loss: 0.2213 - learning_rate: 2.5000e-05\nEpoch 20/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.9011 - loss: 0.4820 - val_accuracy: 0.9411 - val_loss: 0.2076 - learning_rate: 2.5000e-05\nEpoch 21/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 249ms/step - accuracy: 0.9057 - loss: 0.4795 - val_accuracy: 0.9425 - val_loss: 0.1937 - learning_rate: 2.5000e-05\nEpoch 22/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.8900 - loss: 0.4921 - val_accuracy: 0.9467 - val_loss: 0.2016 - learning_rate: 2.5000e-05\nEpoch 23/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 251ms/step - accuracy: 0.9113 - loss: 0.4686 - val_accuracy: 0.9377 - val_loss: 0.2098 - learning_rate: 2.5000e-05\nEpoch 24/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.9220 - loss: 0.4569 - val_accuracy: 0.9446 - val_loss: 0.2027 - learning_rate: 2.5000e-05\nEpoch 25/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.9231 - loss: 0.4614 - val_accuracy: 0.9515 - val_loss: 0.1621 - learning_rate: 2.5000e-05\nEpoch 26/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 250ms/step - accuracy: 0.9143 - loss: 0.4609 - val_accuracy: 0.9557 - val_loss: 0.1678 - learning_rate: 2.5000e-05\nEpoch 27/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 247ms/step - accuracy: 0.9252 - loss: 0.4723 - val_accuracy: 0.9508 - val_loss: 0.1817 - learning_rate: 2.5000e-05\nEpoch 28/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 249ms/step - accuracy: 0.9276 - loss: 0.4527 - val_accuracy: 0.9481 - val_loss: 0.1943 - learning_rate: 2.5000e-05\nEpoch 29/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 223ms/step - accuracy: 0.9189 - loss: 0.4416\nEpoch 29: ReduceLROnPlateau reducing learning rate to 1.249999968422344e-05.\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 249ms/step - accuracy: 0.9189 - loss: 0.4418 - val_accuracy: 0.9515 - val_loss: 0.2016 - learning_rate: 2.5000e-05\nEpoch 30/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 253ms/step - accuracy: 0.9238 - loss: 0.4328 - val_accuracy: 0.9571 - val_loss: 0.1714 - learning_rate: 1.2500e-05\nEpoch 31/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 256ms/step - accuracy: 0.9332 - loss: 0.3993 - val_accuracy: 0.9584 - val_loss: 0.1607 - learning_rate: 1.2500e-05\nEpoch 32/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 255ms/step - accuracy: 0.9276 - loss: 0.4220 - val_accuracy: 0.9591 - val_loss: 0.1664 - learning_rate: 1.2500e-05\nEpoch 33/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 263ms/step - accuracy: 0.9314 - loss: 0.3994 - val_accuracy: 0.9543 - val_loss: 0.1756 - learning_rate: 1.2500e-05\nEpoch 34/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9185 - loss: 0.4195 - val_accuracy: 0.9591 - val_loss: 0.1732 - learning_rate: 1.2500e-05\nEpoch 35/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 231ms/step - accuracy: 0.9270 - loss: 0.4258\nEpoch 35: ReduceLROnPlateau reducing learning rate to 6.24999984211172e-06.\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 257ms/step - accuracy: 0.9270 - loss: 0.4258 - val_accuracy: 0.9584 - val_loss: 0.1774 - learning_rate: 1.2500e-05\nEpoch 36/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 262ms/step - accuracy: 0.9309 - loss: 0.4429 - val_accuracy: 0.9584 - val_loss: 0.1674 - learning_rate: 6.2500e-06\nEpoch 37/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 273ms/step - accuracy: 0.9298 - loss: 0.4198 - val_accuracy: 0.9598 - val_loss: 0.1672 - learning_rate: 6.2500e-06\nEpoch 38/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 264ms/step - accuracy: 0.9302 - loss: 0.4268 - val_accuracy: 0.9605 - val_loss: 0.1692 - learning_rate: 6.2500e-06\nEpoch 39/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 259ms/step - accuracy: 0.9489 - loss: 0.4024 - val_accuracy: 0.9612 - val_loss: 0.1634 - learning_rate: 6.2500e-06\nEpoch 40/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 255ms/step - accuracy: 0.9374 - loss: 0.4111 - val_accuracy: 0.9626 - val_loss: 0.1600 - learning_rate: 6.2500e-06\nEpoch 41/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 259ms/step - accuracy: 0.9340 - loss: 0.4164 - val_accuracy: 0.9626 - val_loss: 0.1587 - learning_rate: 6.2500e-06\nEpoch 42/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.9357 - loss: 0.4079 - val_accuracy: 0.9626 - val_loss: 0.1589 - learning_rate: 6.2500e-06\nEpoch 43/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step - accuracy: 0.9423 - loss: 0.4075\nEpoch 43: ReduceLROnPlateau reducing learning rate to 3.12499992105586e-06.\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9423 - loss: 0.4075 - val_accuracy: 0.9619 - val_loss: 0.1590 - learning_rate: 6.2500e-06\nEpoch 44/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 259ms/step - accuracy: 0.9327 - loss: 0.3976 - val_accuracy: 0.9605 - val_loss: 0.1615 - learning_rate: 3.1250e-06\nEpoch 45/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.9420 - loss: 0.4101 - val_accuracy: 0.9612 - val_loss: 0.1552 - learning_rate: 3.1250e-06\nEpoch 46/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 228ms/step - accuracy: 0.9345 - loss: 0.4044\nEpoch 46: ReduceLROnPlateau reducing learning rate to 1.56249996052793e-06.\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 254ms/step - accuracy: 0.9346 - loss: 0.4043 - val_accuracy: 0.9612 - val_loss: 0.1571 - learning_rate: 3.1250e-06\nEpoch 47/50\n\u001b[1m120/120\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 260ms/step - accuracy: 0.9261 - loss: 0.4195 - val_accuracy: 0.9605 - val_loss: 0.1567 - learning_rate: 1.5625e-06\nEpoch 47: early stopping\nRestoring model weights from the end of the best epoch: 40.\n\nDenseNet169 Training Results:\n==================================================\nValidation Metrics:\n  Accuracy:  0.9626\n  F1 Score:  0.9625\n  Precision: 0.9631\n  Recall:    0.9626\nTest Metrics:\n  Accuracy:  0.9563\n  F1 Score:  0.9568\n  Precision: 0.9618\n  Recall:    0.9563\nâœ… DenseNet169 training completed successfully\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# =============================================================================\n# Grad-CAM Visualization Script for Diabetic Retinopathy Models\n# FULLY REWRITTEN: Avoids Sequential model issues by using Functional API\n# =============================================================================\n\nprint(\"=\"*80)\nprint(\"GRAD-CAM VISUALIZATION SCRIPT - FUNCTIONAL API VERSION\")\nprint(\"=\"*80)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model, Model\nimport os\n\n# =============================================================================\n# Define Class Labels\n# =============================================================================\n\nCLASS_LABELS = {\n    0: 'No DR',\n    1: 'Mild',\n    2: 'Moderate', \n    3: 'Severe',\n    4: 'Proliferative DR'\n}\n\nprint(\"âœ… Class labels defined:\")\nfor idx, label in CLASS_LABELS.items():\n    print(f\"   {idx}: {label}\")\n\n# =============================================================================\n# Cell 1: Grad-CAM Implementation (FUNCTIONAL API APPROACH)\n# =============================================================================\n\ndef get_last_conv_layer_name(model):\n    \"\"\"\n    Automatically find the last convolutional layer in the model\n    \"\"\"\n    # Check main model layers first\n    for layer in reversed(model.layers):\n        if 'Conv' in layer.__class__.__name__:\n            return layer.name\n    \n    # Check inside base model if exists\n    if hasattr(model.layers[0], 'layers'):\n        for layer in reversed(model.layers[0].layers):\n            if 'Conv' in layer.__class__.__name__:\n                return layer.name\n    \n    raise ValueError(\"Could not find convolutional layer.\")\n\n\ndef make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n    \"\"\"\n    Generate Grad-CAM heatmap using direct layer access (NO Sequential issues)\n    This approach extracts the base model and accesses layers directly\n    \"\"\"\n    # Get the base model (assumes model structure: Sequential([base_model, other_layers]))\n    if hasattr(model.layers[0], 'layers'):\n        base_model = model.layers[0]\n    else:\n        base_model = model\n    \n    # Get the target conv layer\n    try:\n        conv_layer = base_model.get_layer(last_conv_layer_name)\n    except:\n        # Fallback if layer not in base model\n        conv_layer = model.get_layer(last_conv_layer_name)\n    \n    # Create a new functional model: input -> [conv_output, final_predictions]\n    # This completely bypasses the Sequential model structure\n    try:\n        # Try to create model using base model structure\n        grad_model = Model(\n            inputs=base_model.input,\n            outputs=[conv_layer.output, base_model.output]\n        )\n        use_base = True\n    except:\n        # Fallback to full model\n        grad_model = Model(\n            inputs=model.input,\n            outputs=[conv_layer.output, model.output]\n        )\n        use_base = False\n    \n    # Compute gradients\n    with tf.GradientTape() as tape:\n        # Forward pass through grad_model\n        if use_base:\n            # Need to complete the forward pass through remaining layers\n            conv_outputs, base_predictions = grad_model(img_array)\n            \n            # Pass through remaining model layers (after base_model)\n            x = base_predictions\n            for layer in model.layers[1:]:\n                x = layer(x)\n            predictions = x\n        else:\n            conv_outputs, predictions = grad_model(img_array)\n        \n        # Get predicted class\n        if pred_index is None:\n            pred_index = tf.argmax(predictions[0])\n        \n        # Get loss for target class\n        class_channel = predictions[:, pred_index]\n    \n    # Compute gradients\n    grads = tape.gradient(class_channel, conv_outputs)\n    \n    if grads is None:\n        raise ValueError(f\"Gradients are None for layer {last_conv_layer_name}\")\n    \n    # Global average pooling\n    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n    \n    # Weight channels and sum\n    conv_outputs = conv_outputs[0]\n    heatmap = conv_outputs @ pooled_grads[..., tf.newaxis]\n    heatmap = tf.squeeze(heatmap)\n    \n    # Normalize\n    heatmap = tf.maximum(heatmap, 0)\n    heatmap = heatmap / (tf.reduce_max(heatmap) + 1e-10)\n    \n    return heatmap.numpy()\n\n\ndef overlay_gradcam(img, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n    \"\"\"\n    Overlay Grad-CAM heatmap on original image\n    \"\"\"\n    # Resize heatmap\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    \n    # Convert to RGB\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, colormap)\n    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n    \n    # Convert image to uint8\n    img_uint8 = np.uint8(255 * img)\n    \n    # Superimpose\n    superimposed_img = heatmap * alpha + img_uint8\n    superimposed_img = np.clip(superimposed_img, 0, 255).astype('uint8')\n    \n    return superimposed_img\n\n\n# =============================================================================\n# Cell 2: Select 50 Images\n# =============================================================================\n\ndef select_images_per_class(x_data, y_data, num_per_class=10, random_seed=42):\n    \"\"\"\n    Select specified number of images per class\n    \"\"\"\n    np.random.seed(random_seed)\n    \n    # Convert one-hot to integer if needed\n    if len(y_data.shape) > 1:\n        labels = np.argmax(y_data, axis=1)\n    else:\n        labels = y_data\n    \n    selected_indices = []\n    \n    for class_idx in range(5):\n        class_indices = np.where(labels == class_idx)[0]\n        \n        if len(class_indices) >= num_per_class:\n            selected = np.random.choice(class_indices, num_per_class, replace=False)\n        else:\n            selected = class_indices\n            print(f\"Warning: Only {len(class_indices)} images for class {class_idx}\")\n        \n        selected_indices.extend(selected)\n    \n    selected_indices = np.array(selected_indices)\n    selected_images = x_data[selected_indices]\n    selected_labels = labels[selected_indices]\n    \n    return selected_indices, selected_images, selected_labels\n\n\nprint(\"\\nðŸ“Š Selecting 50 images (10 per class)...\")\nselected_indices, selected_images, selected_labels = select_images_per_class(\n    x_test, y_test, num_per_class=10, random_seed=42\n)\n\nprint(f\"âœ… Selected {len(selected_indices)} images\")\nprint(f\"   Class distribution: {np.bincount(selected_labels)}\")\n\n\n# =============================================================================\n# Cell 3: Model Configuration\n# =============================================================================\n\nMODEL_PATHS = {\n    'EfficientNetV2M': '/kaggle/working/effnetv2m_final.h5',\n    'DenseNet169': '/kaggle/working/densenet169_best.h5',\n    'InceptionV3': '/kaggle/working/inceptionv3_final.h5',\n    'EfficientNetB5': '/kaggle/working/effnetb5_final.h5'\n}\n\n# Check available models\navailable_models = {}\nfor model_name, model_path in MODEL_PATHS.items():\n    if os.path.exists(model_path):\n        available_models[model_name] = model_path\n        print(f\"âœ… Found: {model_name}\")\n    else:\n        print(f\"âš ï¸  Not found: {model_name}\")\n\nif len(available_models) == 0:\n    print(\"\\nâŒ No models found!\")\nelse:\n    print(f\"\\nâœ… Found {len(available_models)} model(s)\")\n\n\n# =============================================================================\n# Cell 4: Generate Grad-CAM\n# =============================================================================\n\ndef generate_gradcam_for_model(model_name, model_path, images, labels):\n    \"\"\"\n    Generate Grad-CAM visualizations\n    \"\"\"\n    print(f\"\\n{'='*80}\")\n    print(f\"Processing: {model_name}\")\n    print(f\"{'='*80}\")\n    \n    # Load model\n    print(f\"Loading model...\")\n    model = load_model(model_path)\n    print(f\"   âœ“ Model loaded\")\n    \n    # Print model structure for debugging\n    print(f\"   Model type: {type(model)}\")\n    print(f\"   Number of layers: {len(model.layers)}\")\n    if hasattr(model.layers[0], 'layers'):\n        print(f\"   Base model: {type(model.layers[0])}\")\n        print(f\"   Base model layers: {len(model.layers[0].layers)}\")\n    \n    # Find conv layer\n    try:\n        last_conv_layer = get_last_conv_layer_name(model)\n        print(f\"âœ… Using layer: {last_conv_layer}\")\n    except ValueError as e:\n        print(f\"âŒ Error: {e}\")\n        return\n    \n    # Create directories\n    output_dir = f'gradcam_{model_name.lower()}'\n    original_dir = f'{output_dir}/original'\n    overlay_dir = f'{output_dir}/overlay'\n    combined_dir = f'{output_dir}/combined'\n    \n    os.makedirs(original_dir, exist_ok=True)\n    os.makedirs(overlay_dir, exist_ok=True)\n    os.makedirs(combined_dir, exist_ok=True)\n    \n    # Generate visualizations\n    print(f\"\\nðŸ”¥ Generating Grad-CAM...\")\n    \n    success_count = 0\n    error_count = 0\n    \n    for idx, (img, true_label) in enumerate(zip(images, labels)):\n        try:\n            # Prepare image\n            img_array = np.expand_dims(img, axis=0)\n            \n            # Get prediction\n            preds = model.predict(img_array, verbose=0)\n            pred_label = np.argmax(preds[0])\n            confidence = preds[0][pred_label]\n            \n            # Generate Grad-CAM\n            heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer, pred_index=pred_label)\n            gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n            \n            # Filename\n            base_filename = f'{idx+1:02d}_class{true_label}_pred{pred_label}'\n            \n            # Save original\n            fig_orig = plt.figure(figsize=(5, 5))\n            plt.imshow(img)\n            plt.axis('off')\n            plt.tight_layout(pad=0)\n            plt.savefig(f'{original_dir}/{base_filename}_original.png', \n                       dpi=150, bbox_inches='tight', pad_inches=0)\n            plt.close(fig_orig)\n            \n            # Save overlay\n            fig_overlay = plt.figure(figsize=(5, 5))\n            plt.imshow(gradcam_img)\n            plt.axis('off')\n            plt.tight_layout(pad=0)\n            plt.savefig(f'{overlay_dir}/{base_filename}_overlay.png', \n                       dpi=150, bbox_inches='tight', pad_inches=0)\n            plt.close(fig_overlay)\n            \n            # Combined visualization\n            fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n            \n            axes[0].imshow(img)\n            axes[0].set_title(f'Original\\nTrue: {CLASS_LABELS[true_label]}', fontsize=10)\n            axes[0].axis('off')\n            \n            axes[1].imshow(heatmap, cmap='jet')\n            axes[1].set_title('Grad-CAM', fontsize=10)\n            axes[1].axis('off')\n            \n            axes[2].imshow(gradcam_img)\n            axes[2].set_title(f'Overlay\\nPred: {CLASS_LABELS[pred_label]} ({confidence:.2%})', fontsize=10)\n            axes[2].axis('off')\n            \n            correct = \"âœ“\" if true_label == pred_label else \"âœ—\"\n            fig.suptitle(f'{model_name} - Image {idx+1}/50 {correct}', \n                        fontsize=14, fontweight='bold')\n            \n            plt.tight_layout()\n            plt.savefig(f'{combined_dir}/{base_filename}_combined.png', \n                       dpi=150, bbox_inches='tight')\n            plt.close(fig)\n            \n            success_count += 1\n            \n            if (idx + 1) % 10 == 0:\n                print(f\"   âœ“ Processed {idx+1}/50 ({success_count} successful)\")\n                \n        except Exception as e:\n            error_count += 1\n            print(f\"   âš ï¸ Error on image {idx+1}: {str(e)}\")\n            continue\n    \n    print(f\"\\nâœ… Completed: {success_count} successful, {error_count} errors\")\n    print(f\"   Output: '{output_dir}/'\")\n    \n    # Cleanup\n    del model\n    tf.keras.backend.clear_session()\n\n\n# Process all models\nfor model_name, model_path in available_models.items():\n    generate_gradcam_for_model(model_name, model_path, selected_images, selected_labels)\n\n\n# =============================================================================\n# Cell 5: Summary Comparison\n# =============================================================================\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING SUMMARY COMPARISON\")\nprint(\"=\"*80)\n\ndef create_summary_comparison(models_dict, images, labels, num_samples=5):\n    \"\"\"\n    Create model comparison visualizations\n    \"\"\"\n    print(f\"\\nCreating comparison for {num_samples} samples...\")\n    \n    np.random.seed(42)\n    \n    # Select samples\n    sample_indices = []\n    for class_idx in range(min(5, num_samples)):\n        class_mask = labels == class_idx\n        class_imgs = np.where(class_mask)[0]\n        if len(class_imgs) > 0:\n            sample_indices.append(np.random.choice(class_imgs))\n    \n    # Load models\n    loaded_models = {}\n    conv_layers = {}\n    \n    print(\"Loading models...\")\n    for model_name, model_path in models_dict.items():\n        loaded_models[model_name] = load_model(model_path)\n        conv_layers[model_name] = get_last_conv_layer_name(loaded_models[model_name])\n        print(f\"   âœ“ {model_name}\")\n    \n    os.makedirs('gradcam_comparison', exist_ok=True)\n    \n    for sample_idx, img_idx in enumerate(sample_indices):\n        print(f\"   Comparison {sample_idx+1}/{len(sample_indices)}...\")\n        \n        img = images[img_idx]\n        true_label = labels[img_idx]\n        \n        num_models = len(loaded_models)\n        fig, axes = plt.subplots(1, num_models + 1, figsize=(5 * (num_models + 1), 5))\n        \n        # Original\n        axes[0].imshow(img)\n        axes[0].set_title(f'Original\\nTrue: {CLASS_LABELS[true_label]}', \n                         fontsize=11, fontweight='bold')\n        axes[0].axis('off')\n        \n        # Grad-CAM from each model\n        img_array = np.expand_dims(img, axis=0)\n        \n        for idx, (model_name, model) in enumerate(loaded_models.items()):\n            preds = model.predict(img_array, verbose=0)\n            pred_label = np.argmax(preds[0])\n            confidence = preds[0][pred_label]\n            \n            heatmap = make_gradcam_heatmap(img_array, model, conv_layers[model_name], pred_index=pred_label)\n            gradcam_img = overlay_gradcam(img, heatmap, alpha=0.4)\n            \n            axes[idx + 1].imshow(gradcam_img)\n            axes[idx + 1].set_title(f'{model_name}\\n{CLASS_LABELS[pred_label]} ({confidence:.1%})', \n                                   fontsize=11, fontweight='bold')\n            axes[idx + 1].axis('off')\n        \n        fig.suptitle(f'Model Comparison - Sample {sample_idx + 1} (Class: {CLASS_LABELS[true_label]})', \n                    fontsize=14, fontweight='bold')\n        plt.tight_layout()\n        \n        plt.savefig(f'gradcam_comparison/comparison_sample_{sample_idx + 1}.png', \n                   dpi=150, bbox_inches='tight')\n        plt.close()\n    \n    print(f\"âœ… Saved to 'gradcam_comparison/'\")\n    \n    # Cleanup\n    for model in loaded_models.values():\n        del model\n    tf.keras.backend.clear_session()\n\n\n# Create comparison\nif len(available_models) > 1:\n    create_summary_comparison(available_models, selected_images, selected_labels, num_samples=5)\nelif len(available_models) == 1:\n    print(\"âš ï¸  Only 1 model. Skipping comparison.\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… GRAD-CAM COMPLETED!\")\nprint(\"=\"*80)\nprint(f\"\\nðŸ“ Output directories:\")\nfor model_name in available_models.keys():\n    model_dir = f\"gradcam_{model_name.lower()}\"\n    print(f\"   - {model_dir}/\")\nif len(available_models) > 1:\n    print(f\"   - gradcam_comparison/\")\nprint(\"\\nðŸŽ‰ Done!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T08:01:15.554500Z","iopub.execute_input":"2025-10-22T08:01:15.555054Z","iopub.status.idle":"2025-10-22T08:03:14.303409Z","shell.execute_reply.started":"2025-10-22T08:01:15.555028Z","shell.execute_reply":"2025-10-22T08:03:14.302558Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nGRAD-CAM VISUALIZATION SCRIPT - FUNCTIONAL API VERSION\n================================================================================\nâœ… Class labels defined:\n   0: No DR\n   1: Mild\n   2: Moderate\n   3: Severe\n   4: Proliferative DR\n\nðŸ“Š Selecting 50 images (10 per class)...\nâœ… Selected 50 images\n   Class distribution: [10 10 10 10 10]\nâš ï¸  Not found: EfficientNetV2M\nâœ… Found: DenseNet169\nâš ï¸  Not found: InceptionV3\nâš ï¸  Not found: EfficientNetB5\n\nâœ… Found 1 model(s)\n\n================================================================================\nProcessing: DenseNet169\n================================================================================\nLoading model...\n   âœ“ Model loaded\n   Model type: <class 'keras.src.models.sequential.Sequential'>\n   Number of layers: 6\n   Base model: <class 'keras.src.models.functional.Functional'>\n   Base model layers: 595\nâœ… Using layer: conv5_block32_2_conv\n\nðŸ”¥ Generating Grad-CAM...\n   âœ“ Processed 10/50 (10 successful)\n   âœ“ Processed 20/50 (20 successful)\n   âœ“ Processed 30/50 (30 successful)\n   âœ“ Processed 40/50 (40 successful)\n   âœ“ Processed 50/50 (50 successful)\n\nâœ… Completed: 50 successful, 0 errors\n   Output: 'gradcam_densenet169/'\n\n================================================================================\nCREATING SUMMARY COMPARISON\n================================================================================\nâš ï¸  Only 1 model. Skipping comparison.\n\n================================================================================\nâœ… GRAD-CAM COMPLETED!\n================================================================================\n\nðŸ“ Output directories:\n   - gradcam_densenet169/\n\nðŸŽ‰ Done!\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}